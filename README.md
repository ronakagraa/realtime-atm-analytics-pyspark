ATM Big Data Analytics Project (PySpark)

This project demonstrates how PySpark can be used to process and analyze ATM sensor and transaction data at scale. It simulates real-world ATM behavior, detects anomalies such as fraud and overheating, and generates actionable insights.

The entire project runs on Google Colab, removing the need for any local installations such as Java, Spark, Hadoop, or winutils. This ensures a smooth, error-free experience across Windows, macOS, and Linux.

ğŸ§¾ Project Summary

ATMs generate operational and transaction data continuously. This project processes that data using Big Data engineering techniques:

âœ” Data Generation

Synthetic ATM data is created with:

ATM ID

Temperature

Cash levels (â‚¹500 and â‚¹200 notes)

Transaction amount

Timestamp

âœ” Data Processing (PySpark)

Using PySpark, the project performs:

RDD Operations: Basic transformations and filtering (e.g., count high-temperature events)

DataFrame Transformations: Add derived columns such as total_cash

Spark SQL: Fraud detection rules Temperature monitoring ATM performance summaries

âœ” Output

Results are saved in organized CSV folders:

fraud_output/

temp_output/

cash_summary_output/

These files are lightweight and can be uploaded directly to GitHub.

ğŸ’¡ Why Use Google Colab for This Project?

Google Colab is the best platform for PySpark projects for beginners and students because:

â­ 1. No Installation Needed

Local Spark setup often requires:

Java installation

Spark download

Hadoop/winutils configuration (Windows)

Environment variables

These typically cause errors like:

â€œJava not foundâ€ â€œNo winutils.exe presentâ€ â€œSpark context failed to startâ€

Colab solves all of this automatically.

â­ 2. Preconfigured Cloud Environment

Colab provides:

Cloud CPU

Enough memory for large datasets

Ability to run Spark smoothly

â­ 3. Easy GitHub Integration

Colab allows direct saving to GitHub using:

File â†’ Save a copy to GitHub

This makes it perfect for building portfolio-ready Big Data projects.

â­ 4. Platform Independent

Works on:

Windows

macOS

Linux

Mobile (with restrictions)

No installation issues.

ğŸ—ï¸ Project Architecture (High Level) Data Generator â†’ Raw ATM CSV â†’ PySpark Engine â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RDD Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ High Temp Alerts Preprocessing â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DataFrames & SQL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ Fraud Detection Cash Summary â”‚ â”‚ CSV Outputs (GitHub-ready)

â–¶ï¸ How to Run This Project (Google Colab)

Follow these simple steps:

Step 1 â€” Open Google Colab

Go to: https://colab.research.google.com

Click: New Notebook

Step 2 â€” Add Text and Code Cells

Paste the provided project notebook content:

Text â†’ Markdown cells

Code â†’ Code cells

Run cells one by one in order.

Step 3 â€” Generate Data

A synthetic ATM dataset is created (CSV).

Step 4 â€” Run the PySpark Pipeline

The notebook performs:

RDD operations

DataFrame transformations

Spark SQL queries

Step 5 â€” View and Download Results

Once executed, folders like:

fraud_output/

temp_output/

cash_summary_output/

will appear in the Colab file explorer.

You can download them for verification or include them in GitHub.
